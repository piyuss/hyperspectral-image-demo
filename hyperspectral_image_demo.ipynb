{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of local_copy_hyper_demo",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/piyuss/hyperspectral-image-demo/blob/main/hyperspectral_image_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6hT23oY0imQ"
      },
      "source": [
        "#Tutorial: Hyperspectral Image Analysis\n",
        "\n",
        "<p align=\"center\"> <img src= \"https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41598-021-85737-x/MediaObjects/41598_2021_85737_Fig1_HTML.png?as=webp\" width=50%>\n",
        "\n",
        "\n",
        "\n",
        "<p align=\"center\"> <img src= \"https://i2.wp.com/micasense.com/wp-content/uploads/2018/12/0051a-1cdzv0-ekhkg7odew-dorlw.png?resize=780%2C480&ssl=1\" width=50%>\n",
        "\n",
        "\n",
        "**Image source:**\n",
        "1. https://www.mdpi.com/2313-433X/6/5/29\n",
        "2. https://micasense.com/uncovering-spectral-signatures-in-a-pecan-orchard/\n",
        "                  \n",
        "\n",
        "In this tutorial, we will go through steps in hyperspectral image processing using images of lettuces. We will start with a raw binary file and obtain an average spectrum from our region of interest. These are the steps we will follow:\n",
        "\n",
        "\n",
        "1.   Download and import hyperspectral data\n",
        "2.   Inspect the true color composite image\n",
        "3.   Segment the plant pixels\n",
        "4.   Extract average spectrum and calculate reflectance\n",
        "5.   Examine dimension reduction and modeling techniques\n",
        "\n",
        "\n",
        "\n",
        "First, let's download the files we will work with. Hyperspectral image files tend to be large and will take a while to download. Run the code cell below by clicking on the **Play icon** to the left of the cell. You can also run the cell by clicking on the cell and pressing **CTRL/CMD+ENTER**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VHz78UzagVi"
      },
      "source": [
        "# 1 Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XizI9BhU-nqW"
      },
      "source": [
        "#@title 1.1 Download data\n",
        "\n",
        "!wget -O download.zip https://www.dropbox.com/s/ja2zvxg1op1f5jz/images.zip?dl=0\n",
        "!unzip download.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sM0k6_AW4RJ"
      },
      "source": [
        "Let us look at the files that we downloaded. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0M1xhLYHAJU",
        "outputId": "07fd5010-92c8-4e3b-b791-4781683b4754"
      },
      "source": [
        "#@title 1.2 Inspect downloaded files\n",
        "\n",
        "from glob import glob \n",
        "file_list = glob(\"images/\" + \"*.*\", recursive = True)\n",
        "print(\"List of filenames:\", file_list)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List of filenames: []\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD6wl0wiXFGZ"
      },
      "source": [
        "We have two different types of files in the directory:\n",
        "\n",
        "1. BIL files are binary files containing uncompressed image pixel data. In this format, pixel values are simply stored row-wise for each wavelength channel. We need additional information on the number of rows, columns, and bands to import the BIL files.\n",
        "\n",
        "2. HDR files, or header files, contain the information about the BIL files.\n",
        "\n",
        "<p align=\"center\"> <img src= \"https://github.com/piyuss/hyperspectral-image-demo/blob/main/illustrations/bil_format.PNG?raw=true\" width=50%>\n",
        "\n",
        "<p align=\"center\"> <i> Structure of a Band Interleaved by Line (BIL) file\n",
        "\n",
        "\n",
        "**Image source:** https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/bil-bip-and-bsq-raster-files.htm +\n",
        "\n",
        "+ *This link will also be useful to read about other image formats you may come across depending on the type of data and the sensor you are using*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VGMNiZKx-DS"
      },
      "source": [
        "We can look at the size of these files to understand that one is a huge binary file and the other is a text file with metadata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT8VJ8V8x8Rz"
      },
      "source": [
        "#@title 1.3 Inspect downloaded file size\n",
        "import os\n",
        "print(\"The size of\", file_list[0], \" is \", \n",
        "      os.path.getsize (file_list[0]), \"bytes.\")\n",
        "print(\"The size of\", file_list[1], \" is \", \n",
        "      os.path.getsize (file_list[1]), \"bytes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYOFmkF9f8Mc"
      },
      "source": [
        "\n",
        "# 2 Import data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmM_1qlheZjW"
      },
      "source": [
        "Let us look at one of the header files to understand the file format before we read in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9uZh_HtWJFL"
      },
      "source": [
        "#@title 2.1 Import and inspect header file\n",
        "import pandas as pd\n",
        "metadata_list = glob(\"images/\" + \"*.bil.hdr\")\n",
        "header_info = pd.read_table(metadata_list[0], header=None)\n",
        "header_info.columns = [\"metadata\"]\n",
        "print(header_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnYCOU5Xg1X7"
      },
      "source": [
        "The header file contains everything one would like to know about the image. The size of the image can be deduced from lines, samples, and bands. The bit depth and byte order can be important depending on how we decide to read in the data. \n",
        "\n",
        "From the first line of the header file, we learn that the format of the header is ENVI. Detailed information about this format can be found here: https://www.l3harrisgeospatial.com/docs/enviheaderfiles.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H6WsjofoQuj"
      },
      "source": [
        "We can use this information and write a program to read BIL files, but a smarter way is to use existing libraries. For this demonstration, we are going to use the open source Spectral Python (SPy) Library which is designed for hyperspectral image analysis. The documentation for the library is here: http://www.spectralpython.net/\n",
        "\n",
        "The source code is here: https://github.com/spectralpython \n",
        "\n",
        "While the specific functions will be different depending on the library used, the general principles remain the same. SPy has a lot of nice abstractions. The user can read most common hyperspectral image formats with just the filename. Functions are provided for many common image operations so that almost everything can be achieved with a basic knowledge of Python and its libraries for scientific computing such as Numpy. Resources for learning Python are everywhere; one free resource that I recommend is here: https://greenteapress.com/wp/think-python/\n",
        "\n",
        "We will first install SPy using the Python Package Index (PyPi) and import it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nOFCbFfffRY"
      },
      "source": [
        "#@title 2.2 Install Spectral Python\n",
        "!pip install spectral\n",
        "from spectral import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixmQXyJnupZz"
      },
      "source": [
        "To read a file with an ENVI header, SPy provides a special function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL7rOCKntUlV"
      },
      "source": [
        "#@title 2.3 Open hyperspectral file\n",
        "img = envi.open(metadata_list[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poB8YJT9uwsD"
      },
      "source": [
        "You will notice that envi.open() function was unexpectedly fast. This is because the hyperspectral data is not read into the computer's memory; only the metadata in the header file has been read in. However, we can use the object img to extract information abou the image as well as to extract specific channels or pixels.\n",
        "\n",
        "It supports common Numpy methods and operators.\n",
        "\n",
        "We can check the dimensions of the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqRVFUP7um3A"
      },
      "source": [
        "#@title 2.4 Check file dimension\n",
        "img.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rp2PVflbwnBR"
      },
      "source": [
        "We can inspect the image intensity at the 200th row and 1000th column of the 100th channel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbSPifgbv-GB"
      },
      "source": [
        "#@title 2.5 Inspect individual pixel\n",
        "img[200,1000,100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyHywFiXxI-f"
      },
      "source": [
        "Now that we are able to access the numbers in the hyperspectral image cube, we can begin to do more interesting things. Before we start doing that, it is a good idea to read the entire image into memory so that operations can be faster. Until now, the program read values into memory only when specifically requested with the subscript ([ ]) notation. You can check the RAM usage of your Colab session from the menu on the top right now and after we run the next code cell to see that the image is read into memory only when the load() method is used. It is a good idea to be mindful of the size of your data, the computer's memory, and how it is being used when dealing with such large files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkXdFRdSuMO0"
      },
      "source": [
        "#@title 2.6 Load image as an array\n",
        "arr = img.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xndSoPGfr5qD"
      },
      "source": [
        "One more thing we will do is to subsample the data cube. This will create a lighter datacube that will be processed faster. This is not necessary (or desirable) in an actual analysis but will help us move faster in the Colaboratory environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH9zTgO3r4M8"
      },
      "source": [
        "#@title 2.7 Subsample image for faster processing\n",
        "arr = arr[::3,::3,:]\n",
        "arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QAbCp5tfv6Y"
      },
      "source": [
        "# 3 Inspect color image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fbB7rcu_HYZ"
      },
      "source": [
        "We will now look at the image channels that are useful for segmentation and analysis. The dataset that we are working with contains the visible and near infra-red wavelengths, so it is possible to extract the red, green, and blue channels to look at the color image. \n",
        "\n",
        "<p align=\"center\"> <img src= \"https://github.com/piyuss/hyperspectral-image-demo/blob/main/illustrations/spectrum_wiki.png?raw=true\" width = 30% >\n",
        "\n",
        "As we see in the image above, the red, green, and blue wavelengths can be taken around 650 nm, 550 nm, and 450 nm respectively. For more information on the visible spectrum: https://www.britannica.com/science/color/The-visible-spectrum\n",
        "\n",
        "We saw that we can use the subscript operator [ ] to extract particular channels from the image. But where do the wavelengths for the red, green, and blue channels lie? In this case, the answer is in the metadata file.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrlRkXCduNGO"
      },
      "source": [
        "print(header_info[\"metadata\"][17])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaWfz7WQI12Z"
      },
      "source": [
        "If we count or write a function to find the location of the RGB wavelengths, we will find that 450.33 nm is the 47th channel, 549.72 nm is the 122th channel, and 649.64 nm is the 197th channel.\n",
        "\n",
        "We will write a small function to extract these channels and create an RGB image,and another function to display an image in the IPython notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oB0EhLeLruI"
      },
      "source": [
        "#@title 3.1 Functions for working with images\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def show_image(img1):\n",
        "    fig1, ax1 = plt.subplots(figsize=(5, 5))\n",
        "    ax1.imshow(img1, cmap = 'gray')\n",
        "\n",
        "def increase_brightness(img, value=80):\n",
        "    img = img.astype(np.uint8)\n",
        "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
        "    h, s, v = cv2.split(hsv)\n",
        "\n",
        "    lim = 255 - value\n",
        "    v[v > lim] = 255\n",
        "    v[v <= lim] += value\n",
        "\n",
        "    final_hsv = cv2.merge((h, s, v))\n",
        "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
        "    return img\n",
        "\n",
        "def create_rgb(hyper_cube):\n",
        "    rgb_image = hyper_cube[:,:,[197,122,47]]\n",
        "    rgb_image = cv2.normalize(rgb_image, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
        "    rgb_image = rgb_image.astype(np.uint8)\n",
        "    rgb_image = increase_brightness(rgb_image)\n",
        "    return rgb_image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lceIRz2-K1pN"
      },
      "source": [
        "#@title 3.2 Create RGB image and display it\n",
        "rgb_img = create_rgb(arr) \n",
        "print(\"The dimensions of the RGB image are:\", rgb_img.shape)\n",
        "show_image(rgb_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsVFPTTYPdvz"
      },
      "source": [
        "We can similarly extract the NDVI image. NDVI stands for Normalized Difference Vegetation Index and is calculated as the normalized difference between reflectance at the NIR and red-edge wavelength bands. Because of the nature of reflectance of vegetation, plant pixels have higher values for the ratio. It is also indicative of plant traits such as chlorophyll content. In this exercise, we will only use this ratio for the segmentation of plant pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwruylyAPseK"
      },
      "source": [
        "#@title 3.3 Create and display NDVI image\n",
        "def create_ndvi(hyper_cube):\n",
        "    band_750 = hyper_cube[:,:,272]\n",
        "    band_750 = band_750.astype(np.double)\n",
        "    band_705 = hyper_cube[:,:,197]\n",
        "    band_705 = band_705.astype(np.double)\n",
        "\n",
        "    ndvi_image = np.true_divide((band_750-band_705),(band_750+band_705));\n",
        "    ndvi_image = ndvi_image[:,:,0]\n",
        "    \n",
        "    return ndvi_image\n",
        "\n",
        "ndvi_img = create_ndvi(arr)\n",
        "show_image(ndvi_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDZDWuIkgJcI"
      },
      "source": [
        "# 4 Segment vegetation pixels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN1iwY2CRncI"
      },
      "source": [
        "The NDVI image distinctly separates the plant pixels, but we still need to threshold it to get a binary mask. We first look at the histogram of the NDVI image to determine a suitable threshold if it exists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPXNQ9lWQoq0"
      },
      "source": [
        "plt.hist(ndvi_img, histtype=\"step\")\n",
        "plt.title(\"Histogram of NDVI values\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOrIm6DhR2k2"
      },
      "source": [
        "Looking at the histogram, can make an informed guess that taking a threshold around 0.25 will separate the vegetation pixels from the non-vegetation pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mvvHSwBSzcG"
      },
      "source": [
        "#@title 4.1 Threshold NDVI image\n",
        "def visualize_ndvi_thresh(threshold_value):\n",
        "    f, axarr = plt.subplots(1,2, figsize =(10,10))\n",
        "    axarr[0].imshow(rgb_img)\n",
        "    axarr[1].imshow(ndvi_img>threshold_value,cmap=\"gray\")\n",
        "\n",
        "visualize_ndvi_thresh(0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I7RosoiUtn0"
      },
      "source": [
        "While the segmentation is already acceptable, we can see some error near the upper plant. We will experiment with a higher value of NDVI to see if that will give better results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWDDW6R8Rlg_"
      },
      "source": [
        "visualize_ndvi_thresh(0.35)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh14NwmbU7Bx"
      },
      "source": [
        "The estimation of threshold is not necessarily a process of trial and error. We can try using the popular Otsu's algorithm, for example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Iy1MZcXVdCp"
      },
      "source": [
        "#@title 4.2 Automatic thresholding with Otsu's method\n",
        "from skimage import filters\n",
        "otsu_threshold = filters.threshold_otsu(ndvi_img[~np.isnan(ndvi_img)])\n",
        "print(\"The threshold from Otsu's algorithm is\", otsu_threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YGLU2r0fDjV"
      },
      "source": [
        "The Otsu threshold is close to our estimated threshold. In practice, we would test on a few random images to see that the threshold is applicable to the entire set before running the program for the batch. For now, let's work with a threshold of 0.35. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Do_kIaVyYIHM"
      },
      "source": [
        "mask_initial = ndvi_img>0.35\n",
        "show_image(mask_initial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoYYI80chvN8"
      },
      "source": [
        "We still have non-plant pixels that are segmented out, particularly towards the right edge of the image. The removal of these unwanted noise pixels can be done in a number of ways. The segmentation and noise-removal are steps where tools and techniques in traditional computer vision become useful.\n",
        "\n",
        "One way to remove the noise (and keep the two lettuce blobs) is to simply extract the two largest connected components in the binary image. Below, we create a function that returns a 3-D data structure with the n largest blobs, one in each channel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uJaRUvcmM1y"
      },
      "source": [
        "#@title 4.3 Find n largest blobs in binary image\n",
        "\n",
        "def fill_holes(binary_image):\n",
        "    h, w = binary_image.shape[:2]\n",
        "    mask = np.zeros((h+2, w+2), np.uint8)\n",
        "    cv2.floodFill(255*binary_image.astype(np.uint8), mask, (0,0), 255);\n",
        "    mask = cv2.bitwise_not(mask)\n",
        "    return mask\n",
        "  \n",
        "def get_largest_blobs(binary_image, num_blobs):\n",
        "\n",
        "    #find connected components \n",
        "    binary_image = binary_image.astype(np.uint8)\n",
        "    num_comps, output, stats, centroids = cv2.connectedComponentsWithStats(binary_image, connectivity=8)\n",
        "  \n",
        "    sizes = stats[1:, -1]; nb_components = num_comps - 1\n",
        "   \n",
        "    blob_indices = np.argsort(sizes)[::-1][0:num_blobs]\n",
        "  \n",
        "    aggregate_img = np.zeros((binary_image.shape[0], binary_image.shape[1],num_blobs))\n",
        "\n",
        "    centroid_list = centroids[blob_indices+1]\n",
        "    for i in range(num_blobs):\n",
        "        img2 = np.zeros((output.shape))\n",
        "        img2[output == blob_indices[i]+1] = 255\n",
        "        img2 = fill_holes(img2) == 255\n",
        "        aggregate_img[:,:,i] = img2[:-2,:-2]\n",
        "    return aggregate_img, centroid_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBovtwNZn1fo"
      },
      "source": [
        "#@title 4.4 Display images\n",
        "\n",
        "f, axarr = plt.subplots(1,5, figsize =(40,40))\n",
        "axarr[0].imshow(rgb_img, cmap=plt.get_cmap(\"gray\"))\n",
        "axarr[0].set_title('RGB image', fontsize=20)\n",
        "axarr[1].imshow(ndvi_img, cmap=plt.get_cmap(\"gray\"))\n",
        "axarr[1].set_title('NDVI image', fontsize=20)\n",
        "axarr[2].imshow(mask_initial, cmap=plt.get_cmap(\"gray\"))\n",
        "axarr[2].set_title('Thresholded NDVI image', fontsize=20)\n",
        "\n",
        "aggregate_img, centroids = get_largest_blobs(binary_image = mask_initial,\n",
        "                                             num_blobs = 2)\n",
        "\n",
        "axarr[3].imshow(aggregate_img[:,:,0], cmap=plt.get_cmap(\"gray\"))\n",
        "axarr[3].set_title('Individual mask 1', fontsize=20)\n",
        "axarr[4].imshow(aggregate_img[:,:,1], cmap=plt.get_cmap(\"gray\"))\n",
        "axarr[4].set_title('Individual mask 2', fontsize=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AECMwuBwAP7"
      },
      "source": [
        "At this point we have successfully segmented the vegetation pixels and also separated the individual plant images present in the image. These operations can be more involved depending on the level of control in the environment for data acquisition. It would also require more work if the plants were touching each other in the image. We can see here that a well designed data acquisition process can make subsequent steps much easier.\n",
        "\n",
        "Since we have created the masks, we will be able to extract the intensities for each wavelength channel. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XJ0Bt3o_anB"
      },
      "source": [
        "# 5 Extract spectral data\n",
        "\n",
        "The cell below calculates the mean intensity of the plant pixels for each channel in a loop. We can visualize the masked intensity image from the last channel of the datacube."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBhb2Ij6pVGJ"
      },
      "source": [
        "#@title 5.1 Extract spectral data\n",
        "\n",
        "plant1_mask = aggregate_img[:,:,0]\n",
        "b = np.repeat(plant1_mask[:, :, np.newaxis], 462, axis=2)\n",
        "\n",
        "import numpy.ma as ma\n",
        "mask_arr = ma.masked_array(arr, mask = np.uint8(1*(b==0))) \n",
        "\n",
        "spectrum_list = [None] * mask_arr.shape[2]\n",
        "i = 0\n",
        "for i in range(mask_arr.shape[2]):\n",
        "  channel_mask = mask_arr[:,:,i]\n",
        "  spectrum_list[i] = channel_mask.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MEmC2M17uAe"
      },
      "source": [
        "show_image(channel_mask.squeeze(axis=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U1b7FsaVyWg"
      },
      "source": [
        "Next, we will plot the average intensity values for the plant region against the wavelength."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGb_5IgzTF8J"
      },
      "source": [
        "#@title 5.2 Function to create spectral plot\n",
        "\n",
        "def plot_spectrum(spectrum_data, title):\n",
        "    fig1, ax1 = plt.subplots(figsize=(6, 6))\n",
        "    wavelength_list = [float(i) for i in header_info[\"metadata\"][17].split(\"{\")[1].split(\"}\")[0].split(\",\")]\n",
        "    ax1.plot(wavelength_list, spectrum_data)\n",
        "    ax1.set_xlabel(\"Wavelength (nm)\")\n",
        "    plt.title(title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3hORcQ-dhO2"
      },
      "source": [
        "plot_spectrum(spectrum_list, \"Mean intensity values\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4Fq5eqYV7mt"
      },
      "source": [
        "We can see that the resulting curve is not at all representative of the reflectance pattern that we expect from plants. We would expect higher reflectance in the green region, a lower reflectance in the red area and a much higher reflectance in the near-infrared region. We do not see that in the above chart because it is showing us the raw intensity values captured by the sensor. These raw numbers are naturally dependent upon the ambient light. In order to get meaningful values, we need to characterize our incoming light and try to measure the reflected radiation in proportion to the incident radiation at each wavelength.\n",
        "\n",
        "The purpose of the calibration panel is precisely to facilitate this correction of intensities. The panel has reflectance values that are known, often from factory calibration. The concept here is that if we can obtain the intensities captured from a surface of known reflectance, we can derive an idea of the incident light. For example, if a surface has a known 20% reflectance value at 550 nm, and it has an average pixel intensity of 100, we use a linear relationship and assume that a surface with 100% reflectance will have an average pixel intensity of 100/0.2 = 500.\n",
        "\n",
        "To find the average pixel intensity of our calibration panel, we need to find a way to segment it out similar to the process we used for plant pixel segmentation. Often, an empirical approach is useful to find features that enable accurate segmentation of objects. In this case, we found that the calibration panel had a unique combination of intensity values at 550 nm and around 800 nm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQddyd3i_lzC"
      },
      "source": [
        "# 6 Segment calibration panel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aPC2JnDfUbQ"
      },
      "source": [
        "g = arr[:,:,122]\n",
        "band800 = arr[:,:,310]\n",
        "ratio1 = (g/band800).squeeze(axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuwF6ubtgz7S"
      },
      "source": [
        "show_image(ratio1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v7sTZomhtPd"
      },
      "source": [
        "thresholded_standard = np.asarray(ratio1>4)\n",
        "\n",
        "show_image(thresholded_standard)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSKvqJuWiL0v"
      },
      "source": [
        "thresholded_standard, centroid1 = get_largest_blobs(thresholded_standard,1)\n",
        "standard_mask = thresholded_standard.squeeze(axis=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YqyDeIT3idV4"
      },
      "source": [
        "show_image(standard_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji_EIZcEhYIy"
      },
      "source": [
        "#@title 6.1 Extract calibration panel spectrum\n",
        "\n",
        "standard_mask_cube = np.repeat(standard_mask[:, :, np.newaxis], 462, axis=2)\n",
        "\n",
        "import numpy.ma as ma\n",
        "mask_arr_standard = ma.masked_array(arr, mask = np.uint8(1*(standard_mask_cube==0))) \n",
        "\n",
        "standard_spectrum = [None] * mask_arr_standard.shape[2]\n",
        "i = 0\n",
        "for i in range(mask_arr_standard.shape[2]):\n",
        "    channel_mask = mask_arr_standard[:,:,i]\n",
        "    standard_spectrum[i] = channel_mask.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnVMk6OqCOZs"
      },
      "source": [
        "show_image(channel_mask.squeeze(axis=2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XztSo-XdCVwg"
      },
      "source": [
        "arr.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16f5GjpAjcaS"
      },
      "source": [
        "plot_spectrum(standard_spectrum, \"Average intensity of calibration standard\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uiu1rmC9j-NX"
      },
      "source": [
        "We know that the calibration standard has reflectance values around 20% throughout the wavelength range. However, the reflectance values are not constant throughout even for the calibration panel and the best way to conduct radiometric correction is to use the factory calibration data. This is simply the reflectance of the panel at various wavelengths measured during its calibration. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m3NpT2hj6th"
      },
      "source": [
        "#@title 6.2 Download factory calibration data\n",
        "\n",
        "!wget https://raw.githubusercontent.com/piyuss/hyperspectral-image-demo/main/files/calibration.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hWGtjlknM7Y"
      },
      "source": [
        "calibration_file = pd.read_csv(\"calibration.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15nczY29noWJ"
      },
      "source": [
        "calibration_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7r5bMKW4nW_z"
      },
      "source": [
        "calib_spectrum = calibration_file[\"reflectance\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-S3YdeVrUn3"
      },
      "source": [
        "# 7 Calculate reflectance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOQ1rjh0sx_I"
      },
      "source": [
        "plant_reflectance = np.asarray(spectrum_list)/np.asarray(standard_spectrum)*np.asarray(calib_spectrum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP2eYDS9tHhW"
      },
      "source": [
        "plot_spectrum(plant_reflectance, \"Reflectance values\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fqy0v3Uuy4B"
      },
      "source": [
        "At this point, we have reduced our image data to a multivariable dataset where the wavelengths are the variables. We have thus moved from image analysis to multivariate statistics. We now have a multitude of tools for exploration and modeling with this dataset."
      ]
    }
  ]
}